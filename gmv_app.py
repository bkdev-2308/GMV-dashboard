# -*- coding: utf-8 -*-
"""
GMV App - G·ªôp Scraper + Web Dashboard

Mode:
  --gui   : Ch·∫°y Scraper GUI (PyQt6) - m·∫∑c ƒë·ªãnh
  --web   : Ch·∫°y Flask Web Server

Lu·ªìng ho·∫°t ƒë·ªông:
  Scraper (GUI) ‚Üí ghi song song 3 n∆°i:
    1. CSV (local file)
    2. Google Sheet (Raw Data)
    3. SQLite (gmv_dashboard.db)
  Web ‚Üí ch·ªâ sync Deal List (shop_id, cluster, link)
"""
import asyncio
import json
import os
import sys
import re
import csv
import argparse
import sqlite3
from datetime import datetime
from functools import wraps
import threading

# Import Google Sheets
try:
    import gspread
    from google.oauth2.service_account import Credentials
    GSPREAD_AVAILABLE = True
except ImportError:
    GSPREAD_AVAILABLE = False
    print("‚ö†Ô∏è gspread kh√¥ng ƒë∆∞·ª£c c√†i. Ch·∫°y: pip install gspread google-auth")

# Import PostgreSQL helper
try:
    from db_helpers import (
        save_to_postgresql, save_deal_list_to_postgresql, 
        get_gmv_with_deallist, HAS_PSYCOPG2,
        # Multi-session functions
        save_to_postgresql_multi_session, archive_session_data, 
        init_multi_session_tables
    )
except ImportError:
    HAS_PSYCOPG2 = False
    def save_to_postgresql(*args, **kwargs):
        print("‚ö†Ô∏è db_helpers kh√¥ng t√¨m th·∫•y")
        return False
    def save_deal_list_to_postgresql(*args, **kwargs):
        print("‚ö†Ô∏è db_helpers kh√¥ng t√¨m th·∫•y")
        return 0
    def get_gmv_with_deallist(*args, **kwargs):
        print("‚ö†Ô∏è db_helpers kh√¥ng t√¨m th·∫•y")
        return []
    def save_to_postgresql_multi_session(*args, **kwargs):
        print("‚ö†Ô∏è db_helpers kh√¥ng t√¨m th·∫•y")
        return False
    def archive_session_data(*args, **kwargs):
        print("‚ö†Ô∏è db_helpers kh√¥ng t√¨m th·∫•y")
        return False
    def init_multi_session_tables(*args, **kwargs):
        print("‚ö†Ô∏è db_helpers kh√¥ng t√¨m th·∫•y")
        return False

# ============== Common Config ==============

# Th√™m path ƒë·ªÉ import module g·ªëc
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

# Database path
DB_PATH = os.environ.get('DB_PATH', os.path.join(os.path.dirname(__file__), 'gmv_dashboard.db'))

# Service Account Key path
SERVICE_ACCOUNT_KEY = os.path.join(os.path.dirname(__file__), "service-account-key.json")

# Output directory for CSV
OUTPUT_DIR = os.path.join(os.path.dirname(__file__), "out")
os.makedirs(OUTPUT_DIR, exist_ok=True)

# CSV Header m·ªü r·ªông v·ªõi 3 c·ªôt m·ªõi cho Confirmed data
CSV_HEADER_API = [
    "DateTime", "Item ID", "T√™n s·∫£n ph·∫©m",
    "L∆∞·ª£t click tr√™n s·∫£n ph·∫©m", "T·ª∑ l·ªá click v√†o s·∫£n ph·∫©m",
    "T·ªïng ƒë∆°n h√†ng", "C√°c m·∫∑t h√†ng ƒë∆∞·ª£c b√°n", "Doanh thu",
    "T·ª∑ l·ªá click ƒë·ªÉ ƒë·∫∑t h√†ng", "Th√™m v√†o gi·ªè h√†ng",
    "NMV (Confirmed Revenue)", "T·ªïng ƒë∆°n h√†ng (Confirmed)", "C√°c m·∫∑t h√†ng ƒë∆∞·ª£c b√°n (Confirmed)"
]

# Google Sheets scope
SCOPES = [
    'https://www.googleapis.com/auth/spreadsheets',
    'https://www.googleapis.com/auth/drive'
]

# Raw Data Sheet ID - Scraper ghi v√†o ƒë√¢y, Web ƒë·ªçc t·ª´ ƒë√¢y
RAW_DATA_SHEET_ID = os.environ.get('RAW_DATA_SHEET_ID', '1DVnQERNWJWDF3LCxVSsa7nenNppRz9CWZ4dzKiE05Lc')

# Cache config
GMV_CACHE_TTL = 300  # 5 ph√∫t cho GMV data
DEALLIST_CACHE_TTL = 7200  # 2 ti·∫øng cho Deal List

# GMV Cache
_gmv_cache = {
    'data': None,
    'timestamp': None
}

# Deal List Cache (shop_id, cluster mapping)
_deallist_cache = {
    'item_to_shop': {},
    'item_to_cluster': {},
    'timestamp': None
}

# Admin password from environment
ADMIN_PASSWORD = os.environ.get('ADMIN_PASSWORD', 'admin123')

# ============== Database Functions ==============

def get_db():
    """Get database connection"""
    conn = sqlite3.connect(DB_PATH)
    conn.row_factory = sqlite3.Row
    return conn

def init_db():
    """Initialize database tables"""
    conn = get_db()
    cursor = conn.cursor()
    
    # Config table
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS config (
            key TEXT PRIMARY KEY,
            value TEXT
        )
    ''')
    
    # GMV data table
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS gmv_data (
            item_id TEXT PRIMARY KEY,
            item_name TEXT,
            revenue INTEGER,
            shop_id TEXT,
            link_sp TEXT,
            datetime TEXT,
            clicks INTEGER,
            ctr TEXT,
            orders INTEGER,
            items_sold INTEGER,
            confirmed_revenue INTEGER,
            cluster TEXT
        )
    ''')
    
    # Add confirmed_revenue column if not exists
    try:
        cursor.execute('ALTER TABLE gmv_data ADD COLUMN confirmed_revenue INTEGER DEFAULT 0')
    except:
        pass  # Column already exists
    
    # Raw session data table (for monthly analytics)
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS raw_session_data (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            item_name TEXT,
            item_id TEXT,
            revenue INTEGER,
            clicks INTEGER,
            file_name TEXT,
            session_name TEXT
        )
    ''')
    
    conn.commit()
    conn.close()

def get_config(key):
    """Get config value by key"""
    conn = get_db()
    cursor = conn.cursor()
    cursor.execute('SELECT value FROM config WHERE key = ?', (key,))
    row = cursor.fetchone()
    conn.close()
    return row['value'] if row else None

def set_config(key, value):
    """Set config value"""
    conn = get_db()
    cursor = conn.cursor()
    cursor.execute('''
        INSERT OR REPLACE INTO config (key, value) VALUES (?, ?)
    ''', (key, value))
    conn.commit()
    conn.close()

def save_to_sqlite(rows):
    """Ghi d·ªØ li·ªáu tr·ª±c ti·∫øp v√†o SQLite gmv_data table"""
    if not rows:
        return 0
    
    conn = get_db()
    cursor = conn.cursor()
    count = 0
    
    def parse_int(val):
        """Parse value to int, handling various formats"""
        if val is None or val == '':
            return 0
        if isinstance(val, (int, float)):
            return int(val)
        # Remove common formatting characters
        cleaned = str(val).replace(',', '').replace('.', '').replace('‚Ç´', '').replace('%', '').strip()
        try:
            return int(cleaned) if cleaned else 0
        except ValueError:
            try:
                return int(float(cleaned))
            except:
                return 0
    
    for row in rows:
        try:
            # row format from scraper:
            # [0] DateTime, [1] Item ID, [2] T√™n SP, [3] Clicks, [4] CTR, 
            # [5] Orders, [6] Items Sold, [7] Revenue, [8] CTO, [9] ATC,
            # [10] NMV, [11] ConfOrders, [12] ConfItems
            if len(row) < 8:
                print(f"‚ö†Ô∏è Row qu√° ng·∫Øn ({len(row)} cols): {row[:3]}...")
                continue
            
            dt_str = str(row[0]) if row[0] else ''
            item_id = str(row[1]).strip() if row[1] else ''
            item_name = str(row[2]) if row[2] else ''
            clicks = parse_int(row[3])
            ctr = str(row[4]) if row[4] else ''
            orders = parse_int(row[5])
            items_sold = parse_int(row[6])
            revenue = parse_int(row[7])
            confirmed_revenue = parse_int(row[10]) if len(row) > 10 else 0
            
            if not item_id:
                continue
            
            # Debug first few items
            if count < 3:
                print(f"[SQLITE] Inserting: {item_id[:20]}... revenue={revenue}, orders={orders}")
            
            cursor.execute('''
                INSERT OR REPLACE INTO gmv_data 
                (item_id, item_name, revenue, datetime, clicks, ctr, orders, items_sold, confirmed_revenue)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (item_id, item_name, revenue, dt_str, clicks, ctr, orders, items_sold, confirmed_revenue))
            count += 1
            
        except Exception as e:
            print(f"‚ö†Ô∏è L·ªói ghi SQLite row: {e}")
            import traceback
            traceback.print_exc()
            continue
    
    conn.commit()
    
    # Update last sync timestamp
    cursor.execute('''
        INSERT OR REPLACE INTO config (key, value) VALUES (?, ?)
    ''', ('last_sync', datetime.now().isoformat()))
    conn.commit()
    
    conn.close()
    print(f"[SQLITE] Total saved: {count} items")
    return count

# ============== Google Sheets Functions ==============

def get_gspread_client():
    """Get authenticated gspread client"""
    # Try service account key from env (base64) or file
    service_account_json = os.environ.get('GOOGLE_SERVICE_ACCOUNT_JSON')
    
    if service_account_json:
        import base64
        try:
            # Try base64 decode first
            decoded = base64.b64decode(service_account_json).decode('utf-8')
            service_account_info = json.loads(decoded)
        except:
            # If not base64, try direct JSON
            service_account_info = json.loads(service_account_json)
        creds = Credentials.from_service_account_info(service_account_info, scopes=SCOPES)
    else:
        # Local: use service-account-key.json file
        if os.path.exists(SERVICE_ACCOUNT_KEY):
            creds = Credentials.from_service_account_file(SERVICE_ACCOUNT_KEY, scopes=SCOPES)
        else:
            raise Exception("No Google service account credentials found")
    
    return gspread.authorize(creds)

def get_spreadsheet_sheets(spreadsheet_url):
    """Get list of sheet names from spreadsheet"""
    client = get_gspread_client()
    spreadsheet = client.open_by_url(spreadsheet_url)
    return [sheet.title for sheet in spreadsheet.worksheets()]

def get_deallist_mapping(deallist_url=None, deallist_sheet_name=None, force_refresh=False):
    """
    L·∫•y Deal List mapping (item_id -> shop_id, cluster) v·ªõi cache 2 ti·∫øng.
    Returns: (item_to_shop dict, item_to_cluster dict)
    """
    global _deallist_cache
    
    # Tr·∫£ v·ªÅ empty n·∫øu kh√¥ng c√≥ config
    if not deallist_url or not deallist_sheet_name:
        return {}, {}
    
    # Ki·ªÉm tra cache (n·∫øu kh√¥ng force_refresh)
    if not force_refresh and _deallist_cache['timestamp'] is not None:
        cache_age = (datetime.now() - _deallist_cache['timestamp']).total_seconds()
        if cache_age < DEALLIST_CACHE_TTL:
            print(f"[DEALLIST CACHE] Using cached ({cache_age:.0f}s / {DEALLIST_CACHE_TTL}s)")
            return _deallist_cache['item_to_shop'], _deallist_cache['item_to_cluster']
    
    print(f"[DEALLIST] Loading fresh data from Sheet...")
    
    item_to_shop = {}
    item_to_cluster = {}
    
    try:
        client = get_gspread_client()
        spreadsheet = client.open_by_url(deallist_url)
        sheet = spreadsheet.worksheet(deallist_sheet_name)
        
        # L·∫•y full data (A:Z ƒë·ªÉ tr√°nh filter)
        try:
            values = sheet.get('A:Z')
        except:
            values = sheet.get_all_values()
        
        print(f"[DEALLIST] Loaded {len(values)} rows")
        
        # Find header row
        header_idx = 0
        for idx, row in enumerate(values):
            row_text = ' '.join(str(c).lower() for c in row)
            if 'item' in row_text and 'id' in row_text:
                header_idx = idx
                break
        
        if header_idx < len(values):
            headers = values[header_idx]
            data = values[header_idx + 1:]
            
            # Find columns (∆∞u ti√™n finalitemid)
            item_col, shop_col, cluster_col = None, None, None
            for i, h in enumerate(headers):
                h_lower = h.lower().replace(' ', '').replace('_', '')
                if 'finalitemid' in h_lower:
                    item_col = i
                elif item_col is None and 'itemid' in h_lower and 'origin' not in h_lower:
                    item_col = i
                if 'shopid' in h_lower:
                    shop_col = i
                if 'cluster' in h_lower:
                    cluster_col = i
            
            print(f"[DEALLIST] Columns: item={item_col}, shop={shop_col}, cluster={cluster_col}")
            
            if item_col is not None and shop_col is not None:
                for row in data:
                    if len(row) > max(item_col, shop_col):
                        item_id = str(row[item_col]).strip()
                        shop_id_raw = str(row[shop_col]).strip()
                        
                        # Parse shop_id
                        if '+' in shop_id_raw:
                            shop_id = shop_id_raw.split('+')[-1]
                        else:
                            shop_id = shop_id_raw
                        shop_id = ''.join(c for c in shop_id if c.isdigit())
                        
                        if item_id and shop_id:
                            item_to_shop[item_id] = shop_id
                        
                        if cluster_col is not None and len(row) > cluster_col:
                            item_to_cluster[item_id] = str(row[cluster_col]).strip()
                
                print(f"[DEALLIST] Mapped {len(item_to_shop)} items")
        
        # Update cache
        _deallist_cache['item_to_shop'] = item_to_shop
        _deallist_cache['item_to_cluster'] = item_to_cluster
        _deallist_cache['timestamp'] = datetime.now()
        
    except Exception as e:
        print(f"[DEALLIST] Error: {e}")
    
    return item_to_shop, item_to_cluster

def get_gmv_from_sheet(limit=500, deallist_url=None, deallist_sheet_name=None, force_refresh=False):
    """
    ƒê·ªçc GMV data tr·ª±c ti·∫øp t·ª´ Google Sheet (RAW_DATA_SHEET_ID).
    K·∫øt h·ª£p v·ªõi Deal List ƒë·ªÉ l·∫•y shop_id, cluster, link.
    S·ª≠ d·ª•ng cache ƒë·ªÉ tr√°nh g·ªçi API m·ªói request.
    """
    global _gmv_cache
    
    # Ki·ªÉm tra cache (n·∫øu kh√¥ng force_refresh)
    if not force_refresh and _gmv_cache['data'] is not None and _gmv_cache['timestamp'] is not None:
        cache_age = (datetime.now() - _gmv_cache['timestamp']).total_seconds()
        if cache_age < GMV_CACHE_TTL:
            print(f"[GMV CACHE] Using cached ({cache_age:.0f}s / {GMV_CACHE_TTL}s)")
            cached_data = _gmv_cache['data']
            return cached_data[:limit]
    
    print("[CACHE] Fetching fresh data from Google Sheet...")
    
    client = get_gspread_client()
    
    # 1. ƒê·ªçc Raw Data Sheet
    spreadsheet = client.open_by_key(RAW_DATA_SHEET_ID)
    worksheet = spreadsheet.sheet1  # Sheet1
    
    all_values = worksheet.get_all_values()
    if not all_values or len(all_values) < 2:
        return []
    
    header = all_values[0]
    data_rows = all_values[1:]
    
    # 2. L·∫•y Deal List mapping t·ª´ cache
    item_to_shop, item_to_cluster = get_deallist_mapping(deallist_url, deallist_sheet_name, force_refresh=force_refresh)
    
    # 3. Parse GMV data
    # Header expected: DateTime, Item ID, T√™n s·∫£n ph·∫©m, Clicks, CTR, Orders, Items Sold, Revenue, ...
    def find_col(keywords):
        for i, h in enumerate(header):
            h_lower = h.lower().replace(' ', '')
            for kw in keywords:
                if kw in h_lower:
                    return i
        return None
    
    col_item_id = find_col(['itemid'])
    col_item_name = find_col(['t√™nsp', 't√™nsan', 'tensanpham', 's·∫£nph·∫©m'])
    col_revenue = find_col(['doanhthu', 'revenue'])
    col_clicks = find_col(['click', 'l∆∞·ª£tclick'])
    col_ctr = find_col(['t·ª∑l·ªáclick', 'ctr'])
    col_orders = find_col(['t·ªïngƒë∆°n', 'orders', 'ƒë∆°nh√†ng'])
    col_items_sold = find_col(['m·∫∑th√†ng', 'itemssold', 'ƒë∆∞·ª£cb√°n'])
    col_datetime = find_col(['datetime', 'th·ªùigian'])
    
    if col_item_id is None or col_revenue is None:
        # Fallback: use index-based
        col_datetime = 0
        col_item_id = 1
        col_item_name = 2
        col_clicks = 3
        col_ctr = 4
        col_orders = 5
        col_items_sold = 6
        col_revenue = 7
    
    def parse_int(val):
        if val is None or val == '':
            return 0
        cleaned = str(val).replace(',', '').replace('.', '').replace('‚Ç´', '').strip()
        try:
            return int(cleaned) if cleaned else 0
        except:
            return 0
    
    results = []
    seen_items = {}  # Keep last occurrence
    
    for row in data_rows:
        if len(row) <= col_item_id:
            continue
        
        item_id = str(row[col_item_id]).strip() if col_item_id < len(row) else ''
        if not item_id:
            continue
        
        item_name = str(row[col_item_name]) if col_item_name is not None and col_item_name < len(row) else ''
        revenue = parse_int(row[col_revenue]) if col_revenue is not None and col_revenue < len(row) else 0
        clicks = parse_int(row[col_clicks]) if col_clicks is not None and col_clicks < len(row) else 0
        ctr = str(row[col_ctr]) if col_ctr is not None and col_ctr < len(row) else ''
        orders = parse_int(row[col_orders]) if col_orders is not None and col_orders < len(row) else 0
        items_sold = parse_int(row[col_items_sold]) if col_items_sold is not None and col_items_sold < len(row) else 0
        dt_str = str(row[col_datetime]) if col_datetime is not None and col_datetime < len(row) else ''
        
        shop_id = item_to_shop.get(item_id, '')
        cluster = item_to_cluster.get(item_id, '')
        link_sp = f"https://shopee.vn/a-i.{shop_id}.{item_id}" if shop_id else ''
        
        # Overwrite to keep last occurrence
        seen_items[item_id] = {
            'item_id': item_id,
            'item_name': item_name,
            'revenue': revenue,
            'shop_id': shop_id,
            'link_sp': link_sp,
            'datetime': dt_str,
            'clicks': clicks,
            'ctr': ctr,
            'orders': orders,
            'items_sold': items_sold,
            'cluster': cluster
        }
    
    # Sort by revenue desc and limit
    results = list(seen_items.values())
    results.sort(key=lambda x: x['revenue'], reverse=True)
    
    # C·∫≠p nh·∫≠t cache (l∆∞u to√†n b·ªô data, kh√¥ng ch·ªâ limit)
    _gmv_cache['data'] = results
    _gmv_cache['timestamp'] = datetime.now()
    print(f"[CACHE] Updated cache with {len(results)} items")
    
    return results[:limit]

def sync_deal_list_only(spreadsheet_url, deallist_sheet_name):
    """
    CH·ªà sync Deal List ƒë·ªÉ map shop_id, cluster, t·∫°o link.
    KH√îNG x√≥a/ghi ƒë√® gmv_data - ch·ªâ UPDATE c√°c item ƒë√£ c√≥.
    """
    client = get_gspread_client()
    spreadsheet = client.open_by_url(spreadsheet_url)
    
    # Read Deal list - d√πng range get ƒë·ªÉ l·∫•y full data kh√¥ng b·ªã filter
    deallist_sheet = spreadsheet.worksheet(deallist_sheet_name)
    try:
        deallist_values = deallist_sheet.get('A:Z')
    except:
        deallist_values = deallist_sheet.get_all_values()
    
    print(f"[SYNC] Deal List: {len(deallist_values)} rows from {deallist_sheet_name}")
    
    # Find header row
    header_row_idx = 0
    for idx, row in enumerate(deallist_values):
        row_text = ' '.join(str(cell).lower() for cell in row)
        if 'item' in row_text and 'id' in row_text:
            header_row_idx = idx
            break
    
    # Extract headers and data
    if header_row_idx < len(deallist_values):
        headers = deallist_values[header_row_idx]
        data_rows = deallist_values[header_row_idx + 1:]
    else:
        headers = deallist_values[0] if deallist_values else []
        data_rows = deallist_values[1:] if len(deallist_values) > 1 else []
    
    # Convert to list of dicts
    deallist_data = []
    for row in data_rows:
        if any(cell.strip() for cell in row):
            row_dict = {}
            for i, header in enumerate(headers):
                if i < len(row):
                    row_dict[header] = row[i]
            deallist_data.append(row_dict)
    
    # Find column names
    item_id_col = None
    shop_id_col = None
    cluster_col = None
    
    if deallist_data:
        first_row_keys = list(deallist_data[0].keys())
        for key in first_row_keys:
            key_lower = key.lower().replace(' ', '').replace('_', '')
            if 'finalitemid' in key_lower or 'itemid' in key_lower:
                item_id_col = key
            if 'shopid' in key_lower:
                shop_id_col = key
            if 'cluster' in key_lower:
                cluster_col = key
    
    if not item_id_col or not shop_id_col:
        raise Exception("Kh√¥ng t√¨m th·∫•y c·ªôt Item ID ho·∫∑c Shop ID trong Deal List")
    
    # Update gmv_data v·ªõi shop_id, cluster, link
    conn = get_db()
    cursor = conn.cursor()
    updated = 0
    
    for row in deallist_data:
        item_id = str(row.get(item_id_col, '')).strip()
        shop_id_raw = str(row.get(shop_id_col, '')).strip()
        cluster = str(row.get(cluster_col, '')).strip() if cluster_col else ''
        
        # Extract numeric shop_id
        if '+' in shop_id_raw:
            shop_id = shop_id_raw.split('+')[-1]
        else:
            shop_id = shop_id_raw
        shop_id = ''.join(c for c in shop_id if c.isdigit())
        
        if not item_id or not shop_id:
            continue
        
        # Generate link
        link_sp = f"https://shopee.vn/a-i.{shop_id}.{item_id}"
        
        # UPDATE existing record (kh√¥ng INSERT m·ªõi)
        cursor.execute('''
            UPDATE gmv_data 
            SET shop_id = ?, cluster = ?, link_sp = ?
            WHERE item_id = ?
        ''', (shop_id, cluster, link_sp, item_id))
        
        if cursor.rowcount > 0:
            updated += 1
    
    conn.commit()
    conn.close()
    
    # === L∆ØU V√ÄO POSTGRESQL ===
    # Thu th·∫≠p deal_list data ƒë·ªÉ l∆∞u
    postgres_url = os.environ.get('DATABASE_URL', '')
    if postgres_url:
        deal_list_for_postgres = []
        for row in deallist_data:
            item_id = str(row.get(item_id_col, '')).strip()
            shop_id_raw = str(row.get(shop_id_col, '')).strip()
            cluster = str(row.get(cluster_col, '')).strip() if cluster_col else ''
            
            # Extract numeric shop_id
            if '+' in shop_id_raw:
                shop_id = shop_id_raw.split('+')[-1]
            else:
                shop_id = shop_id_raw
            shop_id = ''.join(c for c in shop_id if c.isdigit())
            
            if item_id and shop_id:
                deal_list_for_postgres.append({
                    'item_id': item_id,
                    'shop_id': shop_id,
                    'cluster': cluster
                })
        
        saved_count = save_deal_list_to_postgresql(deal_list_for_postgres, postgres_url)
        print(f"[SYNC] Saved {saved_count} items to PostgreSQL deal_list")
    
    # Update last sync time
    set_config('last_deallist_sync', datetime.now().isoformat())
    
    return updated


# ============== GUI SCRAPER MODE ==============

def run_gui_mode():
    """Kh·ªüi ƒë·ªông GUI Scraper mode"""
    from PyQt6.QtWidgets import (
        QApplication, QWidget, QVBoxLayout, QHBoxLayout, QLineEdit, 
        QPushButton, QComboBox, QLabel, QMessageBox, QTextEdit, QGroupBox, QCheckBox
    )
    from PyQt6.QtCore import Qt
    
    # Import module g·ªëc
    from backup_full_gmv import ShopeeScraperApp
    
    class ShopeeScraperWithGSheet(ShopeeScraperApp):
        """Phi√™n b·∫£n m·ªü r·ªông v·ªõi t√≠nh nƒÉng Google Sheet + SQLite"""
        
        def __init__(self):
            super().__init__()
            self.setWindowTitle("GMV App - Scraper (CSV + Sheet + SQLite)")
            self.resize(500, 400)
            
            # Google Sheet variables
            self.gsheet_client = None
            self.current_spreadsheet = None
            self.current_worksheet = None
            self.gsheet_enabled = False
            self.gsheet_header_written = False
            
            # Initialize DB
            init_db()
            
            # PostgreSQL variables
            self.postgres_enabled = False
            
            # Th√™m UI elements cho Google Sheet
            self._add_gsheet_ui()
            
            # Th√™m UI elements cho PostgreSQL
            self._add_postgres_ui()
        
        def _add_gsheet_ui(self):
            """Th√™m UI elements cho Google Sheet"""
            gsheet_group = QGroupBox("üìä Google Sheet Settings")
            gsheet_layout = QVBoxLayout()
            
            # Row 1: Sheet URL
            url_row = QHBoxLayout()
            url_row.addWidget(QLabel("Sheet URL:"))
            self.gsheet_url_input = QLineEdit()
            self.gsheet_url_input.setPlaceholderText("Paste Google Spreadsheet URL here...")
            url_row.addWidget(self.gsheet_url_input)
            gsheet_layout.addLayout(url_row)
            
            # Row 2: Load button + Sheet selector
            selector_row = QHBoxLayout()
            self.load_sheets_btn = QPushButton("üìÇ Load Sheets")
            self.load_sheets_btn.clicked.connect(self.load_google_sheets)
            selector_row.addWidget(self.load_sheets_btn)
            
            selector_row.addWidget(QLabel("Ch·ªçn Sheet:"))
            self.sheet_selector = QComboBox()
            self.sheet_selector.setMinimumWidth(150)
            selector_row.addWidget(self.sheet_selector)
            
            # Checkbox enable/disable
            self.gsheet_checkbox = QCheckBox("Ghi v√†o Sheet")
            self.gsheet_checkbox.setChecked(False)
            self.gsheet_checkbox.stateChanged.connect(self._on_gsheet_toggle)
            selector_row.addWidget(self.gsheet_checkbox)
            
            gsheet_layout.addLayout(selector_row)
            
            # Status label
            self.gsheet_status = QLabel("‚ö™ Ch∆∞a k·∫øt n·ªëi Google Sheet")
            gsheet_layout.addWidget(self.gsheet_status)
            
            gsheet_group.setLayout(gsheet_layout)
            
            # Th√™m v√†o layout ch√≠nh
            main_layout = self.layout
            main_layout.insertWidget(main_layout.count() - 1, gsheet_group)
            
            # Th√™m log_output n·∫øu ch∆∞a c√≥
            if not hasattr(self, 'log_output') or self.log_output is None:
                self.log_output = QTextEdit()
                self.log_output.setReadOnly(True)
                main_layout.addWidget(self.log_output)
        
        def _add_postgres_ui(self):
            """Th√™m UI elements cho PostgreSQL"""
            from PyQt6.QtWidgets import QGroupBox, QVBoxLayout, QHBoxLayout, QLabel, QLineEdit, QCheckBox
            
            postgres_group = QGroupBox("üêò PostgreSQL (Railway)")
            postgres_layout = QVBoxLayout()
            
            # Row 1: DATABASE_URL input
            url_row = QHBoxLayout()
            url_row.addWidget(QLabel("DATABASE_URL:"))
            self.postgres_url_input = QLineEdit()
            self.postgres_url_input.setPlaceholderText("postgresql://...")
            # L·∫•y t·ª´ environment n·∫øu c√≥
            env_url = os.environ.get('DATABASE_URL', '')
            if env_url:
                self.postgres_url_input.setText(env_url)
            url_row.addWidget(self.postgres_url_input)
            postgres_layout.addLayout(url_row)
            
            # Row 2: Checkbox enable/disable
            self.postgres_checkbox = QCheckBox("Ghi v√†o PostgreSQL")
            self.postgres_checkbox.setChecked(False)
            self.postgres_checkbox.stateChanged.connect(self._on_postgres_toggle)
            postgres_layout.addWidget(self.postgres_checkbox)
            
            # Status label
            self.postgres_status = QLabel("‚ö™ Ch∆∞a b·∫≠t PostgreSQL")
            postgres_layout.addWidget(self.postgres_status)
            
            postgres_group.setLayout(postgres_layout)
            
            # Th√™m v√†o layout ch√≠nh
            main_layout = self.layout
            main_layout.insertWidget(main_layout.count() - 1, postgres_group)
        
        def _on_postgres_toggle(self, state):
            """X·ª≠ l√Ω khi toggle checkbox PostgreSQL"""
            from PyQt6.QtCore import Qt
            self.postgres_enabled = (state == Qt.CheckState.Checked.value)
            if self.postgres_enabled:
                db_url = self.postgres_url_input.text().strip()
                if not db_url:
                    self.log("‚ö†Ô∏è Ch∆∞a nh·∫≠p DATABASE_URL")
                    self.postgres_checkbox.setChecked(False)
                    self.postgres_enabled = False
                    self.postgres_status.setText("‚ö†Ô∏è Ch∆∞a nh·∫≠p DATABASE_URL")
                elif not HAS_PSYCOPG2:
                    self.log("‚ö†Ô∏è psycopg2 ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t")
                    self.postgres_checkbox.setChecked(False)
                    self.postgres_enabled = False
                    self.postgres_status.setText("‚ö†Ô∏è Thi·∫øu psycopg2")
                else:
                    self.postgres_status.setText("‚úÖ ƒê√£ b·∫≠t PostgreSQL")
            else:
                self.postgres_status.setText("‚ö™ Ch∆∞a b·∫≠t PostgreSQL")
        
        def _on_gsheet_toggle(self, state):
            """X·ª≠ l√Ω khi toggle checkbox Google Sheet"""
            self.gsheet_enabled = (state == Qt.CheckState.Checked.value)
            if self.gsheet_enabled and not self.current_worksheet:
                self.log("‚ö†Ô∏è Ch∆∞a ch·ªçn sheet. Vui l√≤ng Load Sheets v√† ch·ªçn sheet tr∆∞·ªõc.")
                self.gsheet_checkbox.setChecked(False)
                self.gsheet_enabled = False
        
        def load_google_sheets(self):
            """K·∫øt n·ªëi Google Sheet v√† load danh s√°ch worksheets"""
            if not GSPREAD_AVAILABLE:
                QMessageBox.warning(self, "L·ªói", "gspread ch∆∞a ƒë∆∞·ª£c c√†i.\nCh·∫°y: pip install gspread google-auth")
                return
            
            url = self.gsheet_url_input.text().strip()
            if not url:
                QMessageBox.warning(self, "L·ªói", "Vui l√≤ng nh·∫≠p URL Google Spreadsheet")
                return
            
            try:
                self.gsheet_status.setText("üîÑ ƒêang k·∫øt n·ªëi...")
                QApplication.processEvents()
                
                if not os.path.exists(SERVICE_ACCOUNT_KEY):
                    QMessageBox.critical(self, "L·ªói", f"Kh√¥ng t√¨m th·∫•y file:\n{SERVICE_ACCOUNT_KEY}")
                    self.gsheet_status.setText("‚ùå Kh√¥ng t√¨m th·∫•y service account key")
                    return
                
                creds = Credentials.from_service_account_file(SERVICE_ACCOUNT_KEY, scopes=SCOPES)
                self.gsheet_client = gspread.authorize(creds)
                self.current_spreadsheet = self.gsheet_client.open_by_url(url)
                
                worksheets = self.current_spreadsheet.worksheets()
                
                self.sheet_selector.clear()
                for ws in worksheets:
                    self.sheet_selector.addItem(ws.title, ws)
                
                if worksheets:
                    self.current_worksheet = worksheets[0]
                    self.sheet_selector.currentIndexChanged.connect(self._on_sheet_changed)
                
                self.gsheet_status.setText(f"‚úÖ ƒê√£ k·∫øt n·ªëi: {self.current_spreadsheet.title}")
                self.log(f"üìä ƒê√£ load {len(worksheets)} sheets t·ª´: {self.current_spreadsheet.title}")
                
            except gspread.exceptions.SpreadsheetNotFound:
                QMessageBox.critical(self, "L·ªói", "Kh√¥ng t√¨m th·∫•y Spreadsheet.\nH√£y ƒë·∫£m b·∫£o ƒë√£ share v·ªõi:\nbigquery-sheets-uploader@beyondk-live-data.iam.gserviceaccount.com")
                self.gsheet_status.setText("‚ùå Spreadsheet kh√¥ng t√¨m th·∫•y")
            except Exception as e:
                QMessageBox.critical(self, "L·ªói", f"L·ªói k·∫øt n·ªëi:\n{e}")
                self.gsheet_status.setText(f"‚ùå L·ªói: {e}")
                self.log(f"‚ùå L·ªói k·∫øt n·ªëi Google Sheet: {e}")
        
        def _on_sheet_changed(self, index):
            """X·ª≠ l√Ω khi ch·ªçn sheet kh√°c"""
            if index >= 0:
                self.current_worksheet = self.sheet_selector.itemData(index)
                self.log(f"üìã ƒê√£ ch·ªçn sheet: {self.current_worksheet.title}")
        
        def save_to_gsheet(self, rows, overwrite=False):
            """
            Ghi d·ªØ li·ªáu v√†o Google Sheet ƒë√£ ch·ªçn trong GUI.
            overwrite=False: Append data m·ªõi v√†o d∆∞·ªõi header (gi·ªØ data c≈©)
            overwrite=True: X√≥a data c≈©, ghi data m·ªõi
            """
            if not self.gsheet_enabled:
                return False
            
            if not rows:
                return False
            
            # Ki·ªÉm tra ƒë√£ ch·ªçn sheet ch∆∞a
            if not self.current_worksheet:
                self.log("‚ö†Ô∏è Ch∆∞a ch·ªçn sheet ƒë·ªÉ ghi d·ªØ li·ªáu")
                return False
            
            try:
                # D√πng sheet ƒë√£ ch·ªçn trong GUI
                worksheet = self.current_worksheet
                
                # Chu·∫©n b·ªã data
                all_rows = [CSV_HEADER_API]  # Lu√¥n c√≥ header
                
                for row in rows:
                    row_data = list(row) if isinstance(row, (list, tuple)) else [str(row)]
                    if len(row_data) < len(CSV_HEADER_API):
                        row_data += [""] * (len(CSV_HEADER_API) - len(row_data))
                    elif len(row_data) > len(CSV_HEADER_API):
                        row_data = row_data[:len(CSV_HEADER_API)]
                    
                    row_data = [str(x) if x is not None else "" for x in row_data]
                    all_rows.append(row_data)
                
                if overwrite:
                    # X√ìA to√†n b·ªô data c≈© v√† ghi m·ªõi
                    worksheet.clear()
                    worksheet.update('A1', all_rows, value_input_option='USER_ENTERED')
                    self.log(f"üìä ƒê√£ GHI ƒê√à {len(rows)} d√≤ng v√†o Google Sheet (x√≥a data c≈©)")
                else:
                    # Append (gi·ªØ data c≈©)
                    worksheet.append_rows(all_rows[1:], value_input_option='USER_ENTERED')  # B·ªè header
                    self.log(f"üìä ƒê√£ TH√äM {len(rows)} d√≤ng v√†o Google Sheet")
                
                return True
                
            except Exception as e:
                self.log(f"‚ùå L·ªói ghi Google Sheet: {e}")
                import traceback
                self.log(traceback.format_exc())
                return False
        
        def save_to_csv_and_db(self, rows):
            """Ghi CSV + SQLite song song (+ Google Sheet n·∫øu enabled)"""
            if not rows:
                return ""
            
            path = self.session_csv_path
            final_path = path
            
            # X√°c ƒë·ªãnh c√≥ c·∫ßn ghi header kh√¥ng
            header_needed = True
            try:
                if os.path.exists(path) and os.path.getsize(path) > 0:
                    header_needed = False
            except Exception:
                header_needed = not os.path.exists(path)
            
            # Chu·∫©n h√≥a d·ªØ li·ªáu
            cols = len(CSV_HEADER_API)
            norm_rows = []
            for r in rows:
                r = list(r) if isinstance(r, (list, tuple)) else [str(r)]
                if len(r) > cols:
                    r = r[:cols]
                elif len(r) < cols:
                    r = r + [""] * (cols - len(r))
                r = [("" if x is None else str(x)) for x in r]
                norm_rows.append(r)
            
            try:
                # === 1. GHI CSV + GOOGLE SHEET (m·ªói 3 cycles = 15 ph√∫t) ===
                # Kh·ªüi t·∫°o cycle counter n·∫øu ch∆∞a c√≥
                if not hasattr(self, 'file_cycle_counter'):
                    self.file_cycle_counter = 0
                self.file_cycle_counter += 1
                
                if self.file_cycle_counter >= 3:
                    # Ghi CSV
                    mode = "w" if header_needed else "a"
                    encoding = "utf-8-sig" if header_needed else "utf-8"
                    with open(path, mode, encoding=encoding, newline="") as f:
                        w = csv.writer(f, quoting=csv.QUOTE_ALL)
                        if header_needed:
                            w.writerow(CSV_HEADER_API)
                        for r in norm_rows:
                            w.writerow(r)
                    self.log(f"‚úÖ ƒê√£ ghi {len(norm_rows)} d√≤ng v√†o CSV")
                    
                    # Ghi Google Sheet
                    if self.gsheet_enabled:
                        self.save_to_gsheet(norm_rows)
                    
                    self.file_cycle_counter = 0  # Reset counter
                else:
                    self.log(f"ÔøΩ CSV/GSheet: ƒê·ª£i th√™m {3 - self.file_cycle_counter} cycles n·ªØa (15p interval)...")
                
                # === 3. GHI SQLITE ===
                sqlite_count = save_to_sqlite(norm_rows)
                self.log(f"üíæ ƒê√£ ghi/c·∫≠p nh·∫≠t {sqlite_count} d√≤ng v√†o SQLite")
                
                # === 4. GHI POSTGRESQL (Multi-Session) ===
                self.log(f"üêò PostgreSQL enabled: {self.postgres_enabled}")
                if self.postgres_enabled:
                    db_url = self.postgres_url_input.text().strip()
                    session_id = getattr(self, 'current_session_id', None)
                    session_title = getattr(self, 'current_session_title', '')
                    
                    if session_id:
                        self.log(f"üêò ƒêang ghi v√†o PostgreSQL (session: {session_id})...")
                        save_to_postgresql_multi_session(
                            norm_rows, db_url, session_id, session_title, 
                            log_func=self.log
                        )
                    else:
                        # Fallback to old method if no session_id
                        self.log(f"üêò ƒêang ghi v√†o PostgreSQL (legacy mode)...")
                        save_to_postgresql(norm_rows, db_url, log_func=self.log)
                
                self.log("‚úÖ Ho√†n th√†nh ghi d·ªØ li·ªáu")
                return final_path
            
            except PermissionError:
                try:
                    ts = datetime.now().strftime("_%H%M%S")
                    alt = path.replace(".csv", f"{ts}.csv")
                    with open(alt, "w", encoding="utf-8-sig", newline="") as f:
                        w = csv.writer(f, quoting=csv.QUOTE_ALL)
                        w.writerow(CSV_HEADER_API)
                        for r in norm_rows:
                            w.writerow(r)
                    final_path = alt
                    self.log(f"‚ö†Ô∏è File ch√≠nh c√≥ th·ªÉ ƒëang m·ªü. ƒê√£ ghi t·∫°m {len(norm_rows)} d√≤ng v√†o: {alt}")
                    
                    # V·∫´n ghi Google Sheet v√† SQLite
                    if self.gsheet_enabled:
                        self.save_to_gsheet(norm_rows)
                    save_to_sqlite(norm_rows)
                    
                    return final_path
                except Exception as e2:
                    import traceback
                    self.log(f"‚ùå L·ªói khi ghi file ph·ª•: {e2!r}")
                    self.log(traceback.format_exc())
                    return ""
            
            except Exception as e:
                import traceback
                self.log(f"‚ùå L·ªói ghi CSV: {e!r}")
                self.log(traceback.format_exc())
                return ""
    
    # Extract data via API function
    async def extract_data_via_api(self, page, session_id):
        """L·∫•y d·ªØ li·ªáu s·∫£n ph·∫©m tr·ª±c ti·∫øp t·ª´ API v·ªõi pageSize=500."""
        results = []
        page_num = 1
        page_size = 500
        total_products = 0
        now = datetime.now()
        dt_str = now.strftime("%d-%m:%H:%M:%S")
        
        self.log(f"üöÄ ƒêang l·∫•y d·ªØ li·ªáu qua API v·ªõi pageSize={page_size}...")
        
        while True:
            api_url = (
                f"https://creator.shopee.vn/supply/api/lm/sellercenter/realtime/dashboard/productList"
                f"?sessionId={session_id}"
                f"&productName="
                f"&productListTimeRange=0"
                f"&sort=desc"
                f"&page={page_num}"
                f"&pageSize={page_size}"
            )
            
            try:
                response = await page.evaluate('''
                    async (url) => {
                        try {
                            const resp = await fetch(url, {
                                credentials: 'include',
                                headers: {
                                    'Accept': 'application/json',
                                }
                            });
                            return await resp.json();
                        } catch(e) {
                            return { error: e.message };
                        }
                    }
                ''', api_url)
                
                if not response:
                    self.log("‚ùå Kh√¥ng nh·∫≠n ƒë∆∞·ª£c response t·ª´ API")
                    break
                
                if "error" in response:
                    self.log(f"‚ùå L·ªói API: {response.get('error')}")
                    break
                
                data = response.get("data", {})
                product_list = data.get("productList", [])
                
                if not product_list:
                    product_list = data.get("list", [])
                if not product_list:
                    product_list = data.get("items", [])
                if not product_list:
                    product_list = data.get("products", [])
                
                total_count = data.get("totalCount", data.get("total", 0))
                
                if not product_list:
                    if page_num == 1:
                        self.log("‚ö†Ô∏è Kh√¥ng c√≥ s·∫£n ph·∫©m n√†o trong phi√™n live n√†y")
                    break
                
                self.log(f"üì¶ Trang {page_num}: L·∫•y ƒë∆∞·ª£c {len(product_list)} s·∫£n ph·∫©m (T·ªïng: {total_count})")
                
                for product in product_list:
                    try:
                        item_id = str(product.get("itemId", ""))
                        name = product.get("title", "")
                        clicks = str(product.get("productClicks", 0))
                        
                        ctr = product.get("ctr", "0%")
                        if not str(ctr).endswith("%"):
                            try:
                                ctr = f"{float(ctr)*100:.1f}%"
                            except:
                                ctr = f"{ctr}%"
                        
                        total_orders = str(product.get("ordersCreated", 0))
                        items_sold = str(product.get("itemSold", 0))
                        
                        revenue_raw = product.get("revenue", 0)
                        if isinstance(revenue_raw, (int, float)):
                            revenue = str(int(revenue_raw))
                        else:
                            revenue = str(revenue_raw).replace("‚Ç´", "").replace(",", "").replace(".", "").strip()
                        
                        cto_rate = product.get("cor", "0%")
                        if not str(cto_rate).endswith("%"):
                            try:
                                cto_rate = f"{float(cto_rate)*100:.1f}%"
                            except:
                                cto_rate = f"{cto_rate}%"
                        
                        add_to_cart = str(product.get("atc", 0))
                        
                        # Confirmed data
                        nmv_raw = product.get("confirmedRevenue", 0)
                        if isinstance(nmv_raw, (int, float)):
                            nmv = str(int(nmv_raw))
                        else:
                            nmv = str(nmv_raw).replace("‚Ç´", "").replace(",", "").replace(".", "").strip()
                        
                        confirmed_orders = str(product.get("confirmedOrderCnt", 0))
                        confirmed_items_sold = str(product.get("ComfirmedItemsold", product.get("confirmedItemSold", 0)))
                        
                        results.append([
                            dt_str, item_id, name,
                            clicks, ctr, total_orders, items_sold,
                            revenue, cto_rate, add_to_cart,
                            nmv, confirmed_orders, confirmed_items_sold,
                        ])
                        total_products += 1
                        
                    except Exception as e:
                        self.log(f"‚ö†Ô∏è L·ªói parse s·∫£n ph·∫©m: {e}")
                        continue
                
                if len(product_list) < page_size:
                    break
                
                page_num += 1
                await asyncio.sleep(0.5)
                
            except Exception as e:
                self.log(f"‚ùå L·ªói khi g·ªçi API trang {page_num}: {e}")
                import traceback
                self.log(traceback.format_exc())
                break
        
        self.log(f"‚úÖ Ho√†n th√†nh l·∫•y d·ªØ li·ªáu qua API: {total_products} s·∫£n ph·∫©m")
        return results
    
    # Patched run_loop
    original_run_loop = ShopeeScraperApp.run_loop
    
    async def patched_run_loop(self):
        """Phi√™n b·∫£n run_loop s·ª≠ d·ª•ng API tr·ª±c ti·∫øp v·ªõi pageSize=500."""
        from playwright.async_api import async_playwright
        
        LOCAL_PATH = os.path.join(os.getenv("LOCALAPPDATA"), "Data All in One", "Dashboard")
        
        if len(self.accounts) == 0:
            self.log("Ch∆∞a c√≥ t√†i kho·∫£n. Vui l√≤ng th√™m t√†i kho·∫£n tr∆∞·ªõc.")
            return

        account = self.accounts[self.account_selector.currentIndex()]
        username, password = account["username"], account["password"]
        session_file = os.path.join(LOCAL_PATH, f"auth_state_{username}.json")
        url = self.live_url_input.text().strip()
        
        # T·∫°o file CSV duy nh·∫•t cho c·∫£ phi√™n
        if not self.session_csv_path:
            live_id_part = ""
            if "dashboard/live/" in url:
                live_id_part = "_" + url.split("dashboard/live/")[-1].split("/")[0]
            start_time_str = datetime.now().strftime("%Y%m%d_%H%M%S")
            self.session_csv_path = os.path.join(
                OUTPUT_DIR,
                f"SHP_live_session{live_id_part}_{start_time_str}.csv"
            )
        
        # Chu·∫©n b·ªã session
        if not os.path.exists(session_file):
            self.log("Ch∆∞a c√≥ session ‚Üí m·ªü tr√¨nh duy·ªát ƒë·ªÉ ƒëƒÉng nh·∫≠p.")
            async with async_playwright() as p:
                browser = await p.chromium.launch(headless=False)
                context = await browser.new_context()
                page = await context.new_page()
                try:
                    await page.goto("https://creator.shopee.vn", timeout=90_000)
                except Exception:
                    pass
                self.log("H√£y ƒëƒÉng nh·∫≠p v√† v√†o ƒë∆∞·ª£c dashboard/home, sau ƒë√≥ ch·ªù app l∆∞u session...")
                start = datetime.now()
                while (datetime.now() - start).total_seconds() < 300:
                    try:
                        if any(k in page.url for k in ("dashboard", "home")):
                            break
                    except Exception:
                        pass
                    await asyncio.sleep(1)
                await context.storage_state(path=session_file)
                await browser.close()
            self.log("üíæ ƒê√£ l∆∞u session.")

        # D√πng session ƒë·ªÉ scrape
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=False)
            context = await browser.new_context(storage_state=session_file)
            page = await context.new_page()

            dashboard_page = None
            try:
                await page.goto("https://creator.shopee.vn", timeout=90_000)
            except Exception:
                pass

            if url:
                try:
                    await page.goto(url, timeout=90_000)
                    dashboard_page = page
                except Exception as e:
                    self.log(f"‚ùå L·ªói khi m·ªü URL phi√™n live: {e!r}")

            if not dashboard_page:
                dashboard_page = await self.find_dashboard_tab(context, url or "")
            if not dashboard_page:
                self.log("Kh√¥ng t√¨m th·∫•y dashboard. K·∫øt th√∫c.")
                await context.close()
                await browser.close()
                return

            self.log(f"üìÑ Dashboard: {dashboard_page.url}")
            
            # L·∫•y session_id t·ª´ URL
            session_id = None
            current_url = dashboard_page.url
            if "dashboard/live/" in current_url:
                session_id = current_url.split("dashboard/live/")[-1].split("/")[0].split("?")[0]
            
            if not session_id:
                self.log("‚ùå Kh√¥ng t√¨m th·∫•y session_id trong URL. S·ª≠ d·ª•ng ph∆∞∆°ng ph√°p c≈©...")
                await original_run_loop(self)
                return
            
            self.log(f"üîë Session ID: {session_id}")
            
            # Store session_id as instance attribute
            self.current_session_id = session_id
            
            # Fetch session info t·ª´ API ƒë·ªÉ l·∫•y session title
            session_title = ""
            try:
                session_info_url = f"https://creator.shopee.vn/supply/api/lm/sellercenter/realtime/dashboard/sessionInfo?sessionId={session_id}"
                session_info = await dashboard_page.evaluate('''
                    async (url) => {
                        try {
                            const resp = await fetch(url, {credentials: 'include'});
                            return await resp.json();
                        } catch(e) {
                            return {error: e.message};
                        }
                    }
                ''', session_info_url)
                
                if session_info and session_info.get('code') == 0:
                    data = session_info.get('data', {})
                    session_title = data.get('sessionTitle', '') or data.get('title', '') or ''
                    self.log(f"üì∫ Session Title (API): {session_title}")
                else:
                    self.log(f"‚ö†Ô∏è Kh√¥ng l·∫•y ƒë∆∞·ª£c session title, s·ª≠ d·ª•ng ID")
            except Exception as e:
                self.log(f"‚ö†Ô∏è L·ªói fetch session info: {e}")
            
            # Override session_title with Google Sheet name if available
            try:
                spreadsheet_url = get_config('spreadsheet_url')
                if spreadsheet_url:
                    from db_helpers import parse_sheet_title, get_gspread_client
                    client = get_gspread_client()
                    if client:
                        spreadsheet = client.open_by_url(spreadsheet_url)
                        sheet_title = spreadsheet.title
                        parsed_title = parse_sheet_title(sheet_title)
                        if parsed_title:
                            self.log(f"üìã Sheet Title: {sheet_title}")
                            self.log(f"‚ú® Parsed Title: {parsed_title}")
                            session_title = parsed_title
            except Exception as e:
                self.log(f"‚ö†Ô∏è Kh√¥ng l·∫•y ƒë∆∞·ª£c sheet title: {e}")
            
            self.current_session_title = session_title or f"Session {session_id}"
            
            # Initialize multi-session schema
            if self.postgres_enabled:
                db_url = self.postgres_url_input.text().strip()
                init_multi_session_tables(db_url, log_func=self.log)
            
            # Archive tracking
            last_archive_time = datetime.now()
            ARCHIVE_INTERVAL_MINS = 60  # Archive every 60 minutes

            # V√≤ng scrape s·ª≠ d·ª•ng API
            self.is_running = True
            try:
                while self.is_running:
                    try:
                        cycle_start = datetime.now()
                        
                        data = await extract_data_via_api(self, dashboard_page, session_id)
                        
                        if data:
                            saved = self.save_to_csv_and_db(data)
                            if saved:
                                self.log(f"‚úÖ Ho√†n th√†nh ghi d·ªØ li·ªáu")
                        else:
                            self.log("‚ö†Ô∏è Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ ghi")
                        
                        # Check if need to archive (after 60 minutes)
                        elapsed_since_archive = (datetime.now() - last_archive_time).total_seconds() / 60
                        if elapsed_since_archive >= ARCHIVE_INTERVAL_MINS:
                            self.log(f"üì¶ ƒê√£ ƒë·ªß {ARCHIVE_INTERVAL_MINS} ph√∫t, b·∫Øt ƒë·∫ßu archive...")
                            if self.postgres_enabled:
                                db_url = self.postgres_url_input.text().strip()
                                archive_session_data(db_url, session_id, log_func=self.log)
                            last_archive_time = datetime.now()
                            self.log(f"‚úÖ Archive xong, reset timer")

                        # Ch·ªù t·ªõi m·ªëc 5 ph√∫t
                        target_secs = 5 * 60
                        elapsed = (datetime.now() - cycle_start).total_seconds()
                        wait_secs = max(0, int(target_secs - elapsed))

                        self.log(f"‚è±Ô∏è S·∫Ω c·∫≠p nh·∫≠t l·∫°i sau {wait_secs//60} ph√∫t...")
                        for _ in range(wait_secs):
                            if not self.is_running:
                                break
                            await asyncio.sleep(1)
                    except Exception as e:
                        import traceback
                        self.log(f"‚ùå L·ªói trong v√≤ng scrape: {e!r}")
                        self.log(traceback.format_exc())
                        await asyncio.sleep(2)
            finally:
                self.log("ƒê√≥ng tr√¨nh duy·ªát...")
                await context.close()
                await browser.close()
                self.log("ƒê√£ d·ª´ng theo d√µi.")
    
    # G√°n c√°c ph∆∞∆°ng th·ª©c
    ShopeeScraperWithGSheet.extract_data_via_api = extract_data_via_api
    ShopeeScraperWithGSheet.run_loop = patched_run_loop
    ShopeeScraperWithGSheet.save_to_csv_api = ShopeeScraperWithGSheet.save_to_csv_and_db
    
    # Ch·∫°y app
    app = QApplication(sys.argv)
    window = ShopeeScraperWithGSheet()
    window.show()
    sys.exit(app.exec())


# ============== WEB SERVER MODE ==============

def create_app():
    """T·∫°o Flask app - d√πng cho c·∫£ gunicorn v√† local development"""
    from flask import Flask, render_template, request, jsonify, session, redirect, url_for
    from apscheduler.schedulers.background import BackgroundScheduler
    
    app = Flask(__name__)
    app.secret_key = os.environ.get('SECRET_KEY', 'dev-secret-key-change-in-production')
    
    # Initialize DB
    try:
        init_db()
    except Exception as e:
        print(f"[WARNING] init_db failed: {e}")
    
    # ============== Background Auto-Refresh GMV ==============
    # DISABLED: Scraper local now writes directly to PostgreSQL
    # The scheduler was reading from Google Sheet and overwriting PostgreSQL data
    scheduler = None
    
    # def auto_refresh_gmv():
    #     """Background job: t·ª± ƒë·ªông refresh GMV cache m·ªói 5 ph√∫t"""
    #     try:
    #         deallist_url = get_config('spreadsheet_url')
    #         deallist_sheet = get_config('deallist_sheet')
    #         
    #         if deallist_url and deallist_sheet:
    #             print(f"[AUTO-REFRESH] Starting GMV refresh at {datetime.now()}")
    #             data = get_gmv_from_sheet(
    #                 limit=500,
    #                 deallist_url=deallist_url,
    #                 deallist_sheet_name=deallist_sheet,
    #                 force_refresh=True
    #             )
    #             print(f"[AUTO-REFRESH] Completed: {len(data)} items cached")
    #         else:
    #             print("[AUTO-REFRESH] Skipped: Deal List config not set")
    #     except Exception as e:
    #         print(f"[AUTO-REFRESH] Error: {e}")
    
    # Scheduler DISABLED - GMV data comes from local scraper -> PostgreSQL
    # try:
    #     from apscheduler.schedulers.background import BackgroundScheduler
    #     scheduler = BackgroundScheduler()
    #     scheduler.add_job(auto_refresh_gmv, 'interval', minutes=5, id='auto_refresh_gmv')
    #     scheduler.start()
    #     print("[SCHEDULER] Background scheduler started with auto-refresh GMV every 5 min")
    # except Exception as e:
    #     print(f"[WARNING] Scheduler failed to start: {e}")
    
    print("[INFO] GMV data comes from local scraper -> PostgreSQL (scheduler disabled)")

    
    auto_sync_state = {
        'running': False,
        'job_id': None,
        'last_auto_sync': None,
        'next_sync': None
    }
    
    def auto_sync_job():
        """Background job that syncs Deal List every 5 minutes"""
        now = datetime.now()
        
        try:
            spreadsheet_url = get_config('spreadsheet_url')
            deallist_sheet = get_config('deallist_sheet')
            
            if spreadsheet_url and deallist_sheet:
                count = sync_deal_list_only(spreadsheet_url, deallist_sheet)
                auto_sync_state['last_auto_sync'] = now.isoformat()
                print(f"[AUTO-SYNC] {now.strftime('%H:%M:%S')} - Updated {count} products with Deal List")
            else:
                print(f"[AUTO-SYNC] Missing configuration. Skipping sync.")
        except Exception as e:
            print(f"[AUTO-SYNC] Error: {str(e)}")
        
        auto_sync_state['next_sync'] = (now.replace(second=0, microsecond=0) + 
                                         __import__('datetime').timedelta(seconds=300)).isoformat()
    
    def start_auto_sync():
        """Start the auto-sync scheduler"""
        if scheduler is None:
            return False
        
        if auto_sync_state['job_id']:
            stop_auto_sync()
        
        auto_sync_state['running'] = True
        auto_sync_job()
        
        job = scheduler.add_job(
            auto_sync_job,
            'interval',
            seconds=300,
            id='auto_sync_job'
        )
        auto_sync_state['job_id'] = job.id
        return True
    
    def stop_auto_sync():
        """Stop the auto-sync scheduler"""
        if auto_sync_state['job_id'] and scheduler:
            try:
                scheduler.remove_job(auto_sync_state['job_id'])
            except:
                pass
        
        auto_sync_state['running'] = False
        auto_sync_state['job_id'] = None
        auto_sync_state['next_sync'] = None
    
    # ============== Auth Decorator ==============
    def admin_required(f):
        @wraps(f)
        def decorated_function(*args, **kwargs):
            if not session.get('is_admin'):
                return redirect(url_for('admin_login'))
            return f(*args, **kwargs)
        return decorated_function
    
    # ============== Routes ==============
    @app.route('/')
    def index():
        return render_template('index.html')
    
    @app.route('/analytics')
    def analytics():
        return render_template('analytics.html')
    
    @app.route('/admin/login', methods=['GET', 'POST'])
    def admin_login():
        error = None
        if request.method == 'POST':
            password = request.form.get('password', '')
            if password == ADMIN_PASSWORD:
                session['is_admin'] = True
                return redirect(url_for('admin'))
            else:
                error = 'M·∫≠t kh·∫©u kh√¥ng ƒë√∫ng'
        return render_template('admin_login.html', error=error)
    
    @app.route('/admin/logout')
    def admin_logout():
        session.pop('is_admin', None)
        return redirect(url_for('index'))
    
    @app.route('/admin')
    @admin_required
    def admin():
        config = {
            'spreadsheet_url': get_config('spreadsheet_url') or '',
            'rawdata_sheet': get_config('rawdata_sheet') or '',
            'deallist_sheet': get_config('deallist_sheet') or '',
            'last_sync': get_config('last_deallist_sync') or 'Ch∆∞a sync'
        }
        return render_template('admin.html', config=config)
    
    # ============== API Routes ==============
    @app.route('/api/top-gmv')
    def api_top_gmv():
        limit = request.args.get('limit', 500, type=int)
        
        # ƒê·ªçc t·ª´ PostgreSQL v·ªõi JOIN deal_list
        db_url = os.environ.get('DATABASE_URL', '')
        
        if db_url:
            try:
                # D√πng function m·ªõi ƒë·ªçc v·ªõi JOIN deal_list
                data = get_gmv_with_deallist(db_url, limit=limit)
                
                if data:
                    return jsonify({
                        'success': True,
                        'data': data,
                        'count': len(data),
                        'source': 'postgresql_with_deallist',
                        'last_sync': get_config('last_deallist_sync') or datetime.now().isoformat()
                    })
            except Exception as e:
                print(f"[API] PostgreSQL error: {e}")
        
        # Fallback: ƒê·ªçc t·ª´ Google Sheet
        deallist_url = get_config('spreadsheet_url')
        deallist_sheet = get_config('deallist_sheet')
        
        try:
            data = get_gmv_from_sheet(
                limit=limit,
                deallist_url=deallist_url,
                deallist_sheet_name=deallist_sheet
            )
            
            return jsonify({
                'success': True,
                'data': data,
                'count': len(data),
                'source': 'google_sheet',
                'last_sync': datetime.now().isoformat()
            })
        except Exception as e:
            print(f"[API] Error reading from Sheet: {e}")
            # Fallback to SQLite if Sheet fails
            try:
                conn = get_db()
                cursor = conn.cursor()
                cursor.execute('''
                    SELECT item_id, item_name, revenue, shop_id, link_sp, datetime, clicks, ctr, orders, items_sold, cluster
                    FROM gmv_data
                    ORDER BY revenue DESC
                    LIMIT ?
                ''', (limit,))
                
                rows = cursor.fetchall()
                conn.close()
                
                data = []
                for row in rows:
                    data.append({
                        'item_id': row['item_id'],
                        'item_name': row['item_name'],
                        'revenue': row['revenue'],
                        'shop_id': row['shop_id'],
                        'link_sp': row['link_sp'],
                        'datetime': row['datetime'],
                        'clicks': row['clicks'],
                        'ctr': row['ctr'],
                        'orders': row['orders'],
                        'items_sold': row['items_sold'],
                        'cluster': row['cluster']
                    })
                
                return jsonify({
                    'success': True,
                    'data': data,
                    'count': len(data),
                    'source': 'sqlite_fallback',
                    'error': str(e),
                    'last_sync': get_config('last_deallist_sync')
                })
            except Exception as e2:
                return jsonify({
                    'success': False,
                    'error': f'Sheet error: {e}, SQLite error: {e2}'
                }), 500
    
    @app.route('/api/refresh-gmv', methods=['POST'])
    @admin_required
    def api_refresh_gmv():
        """Force refresh GMV cache"""
        try:
            deallist_url = get_config('spreadsheet_url')
            deallist_sheet = get_config('deallist_sheet')
            
            data = get_gmv_from_sheet(
                limit=500,
                deallist_url=deallist_url,
                deallist_sheet_name=deallist_sheet,
                force_refresh=True
            )
            
            return jsonify({
                'success': True,
                'message': f'ƒê√£ refresh {len(data)} items t·ª´ GMV Sheet',
                'count': len(data),
                'cache_ttl': GMV_CACHE_TTL,
                'timestamp': datetime.now().isoformat()
            })
        except Exception as e:
            return jsonify({'success': False, 'error': str(e)}), 500
    
    @app.route('/api/refresh-deallist', methods=['POST'])
    @admin_required
    def api_refresh_deallist():
        """Force refresh Deal List cache + clear GMV cache"""
        global _gmv_cache
        try:
            deallist_url = get_config('spreadsheet_url')
            deallist_sheet = get_config('deallist_sheet')
            
            item_to_shop, item_to_cluster = get_deallist_mapping(
                deallist_url=deallist_url,
                deallist_sheet_name=deallist_sheet,
                force_refresh=True
            )
            
            # Clear GMV cache ƒë·ªÉ l·∫ßn load ti·∫øp theo s·∫Ω apply mapping m·ªõi
            _gmv_cache['data'] = None
            _gmv_cache['timestamp'] = None
            
            return jsonify({
                'success': True,
                'message': f'ƒê√£ refresh {len(item_to_shop)} items t·ª´ Deal List. GMV cache ƒë√£ x√≥a.',
                'count': len(item_to_shop),
                'cache_ttl': DEALLIST_CACHE_TTL,
                'gmv_cache_cleared': True,
                'timestamp': datetime.now().isoformat()
            })
        except Exception as e:
            return jsonify({'success': False, 'error': str(e)}), 500
    
    @app.route('/api/cache-status')
    @admin_required
    def api_cache_status():
        """Get cache status"""
        now = datetime.now()
        
        gmv_age = None
        deallist_age = None
        
        if _gmv_cache['timestamp']:
            gmv_age = (now - _gmv_cache['timestamp']).total_seconds()
        if _deallist_cache['timestamp']:
            deallist_age = (now - _deallist_cache['timestamp']).total_seconds()
        
        return jsonify({
            'gmv': {
                'has_data': _gmv_cache['data'] is not None,
                'count': len(_gmv_cache['data']) if _gmv_cache['data'] else 0,
                'age_seconds': gmv_age,
                'ttl': GMV_CACHE_TTL,
                'last_update': _gmv_cache['timestamp'].isoformat() if _gmv_cache['timestamp'] else None
            },
            'deallist': {
                'count': len(_deallist_cache['item_to_shop']),
                'age_seconds': deallist_age,
                'ttl': DEALLIST_CACHE_TTL,
                'last_update': _deallist_cache['timestamp'].isoformat() if _deallist_cache['timestamp'] else None
            }
        })
    @app.route('/api/sheets', methods=['POST'])
    @admin_required
    def api_get_sheets():
        data = request.get_json()
        spreadsheet_url = data.get('spreadsheet_url', '')
        deallist_sheet = data.get('deallist_sheet', '')
        
        if not spreadsheet_url:
            return jsonify({'success': False, 'error': 'URL kh√¥ng ƒë∆∞·ª£c ƒë·ªÉ tr·ªëng'})
        
        # N·∫øu c√≥ deallist_sheet, l∆∞u c·∫£ 2 v√†o config
        if deallist_sheet:
            set_config('spreadsheet_url', spreadsheet_url)
            set_config('deallist_sheet', deallist_sheet)
            print(f"[CONFIG] Saved: URL={spreadsheet_url[:50]}... Sheet={deallist_sheet}")
        
        try:
            sheets = get_spreadsheet_sheets(spreadsheet_url)
            return jsonify({'success': True, 'sheets': sheets})
        except Exception as e:
            return jsonify({'success': False, 'error': str(e)})
    
    @app.route('/api/sync-deallist', methods=['POST'])
    @admin_required
    def api_sync_deallist():
        """API: Sync Deal List only (map shop_id, cluster, link)"""
        data = request.get_json()
        spreadsheet_url = data.get('spreadsheet_url', '')
        deallist_sheet = data.get('deallist_sheet', '')
        
        if not spreadsheet_url or not deallist_sheet:
            return jsonify({'success': False, 'error': 'Vui l√≤ng ƒëi·ªÅn ƒë·∫ßy ƒë·ªß th√¥ng tin'})
        
        try:
            # Save config
            set_config('spreadsheet_url', spreadsheet_url)
            set_config('deallist_sheet', deallist_sheet)
            
            # Sync Deal List only
            count = sync_deal_list_only(spreadsheet_url, deallist_sheet)
            
            return jsonify({
                'success': True,
                'message': f'ƒê√£ c·∫≠p nh·∫≠t {count} s·∫£n ph·∫©m v·ªõi Deal List',
                'count': count
            })
        except Exception as e:
            return jsonify({'success': False, 'error': str(e)})
    
    @app.route('/api/auto-sync/status')
    @admin_required
    def api_auto_sync_status():
        return jsonify({
            'success': True,
            'running': auto_sync_state['running'],
            'last_auto_sync': auto_sync_state['last_auto_sync'],
            'next_sync': auto_sync_state['next_sync']
        })
    
    @app.route('/api/auto-sync/start', methods=['POST'])
    @admin_required
    def api_auto_sync_start():
        spreadsheet_url = get_config('spreadsheet_url')
        deallist_sheet = get_config('deallist_sheet')
        
        if not spreadsheet_url or not deallist_sheet:
            return jsonify({'success': False, 'error': 'Ch∆∞a c·∫•u h√¨nh. Vui l√≤ng sync th·ªß c√¥ng tr∆∞·ªõc.'})
        
        result = start_auto_sync()
        if not result:
            return jsonify({'success': False, 'error': 'Scheduler ch∆∞a s·∫µn s√†ng.'})
        
        return jsonify({
            'success': True,
            'message': 'Auto-sync Deal List ƒë√£ b·∫Øt ƒë·∫ßu. M·ªói 5 ph√∫t.'
        })
    
    @app.route('/api/auto-sync/stop', methods=['POST'])
    @admin_required
    def api_auto_sync_stop():
        stop_auto_sync()
        return jsonify({'success': True, 'message': 'Auto-sync ƒë√£ d·ª´ng'})
    
    @app.route('/api/config')
    @admin_required
    def api_config():
        return jsonify({
            'success': True,
            'config': {
                'spreadsheet_url': get_config('spreadsheet_url') or '',
                'deallist_sheet': get_config('deallist_sheet') or '',
                'last_sync': get_config('last_deallist_sync') or 'Ch∆∞a sync'
            }
        })
    
    @app.route('/api/analytics/category-distribution')
    def api_category_distribution():
        conn = get_db()
        cursor = conn.cursor()
        cursor.execute('''
            SELECT cluster, SUM(revenue) as total_revenue, COUNT(*) as product_count
            FROM gmv_data
            WHERE cluster IS NOT NULL AND cluster != ''
            GROUP BY cluster
            ORDER BY total_revenue DESC
        ''')
        
        rows = cursor.fetchall()
        conn.close()
        
        data = []
        for row in rows:
            data.append({
                'cluster': row['cluster'],
                'revenue': row['total_revenue'],
                'count': row['product_count']
            })
        
        return jsonify({'success': True, 'data': data})
    
    @app.route('/api/analytics/top-products')
    def api_top_products():
        conn = get_db()
        cursor = conn.cursor()
        cursor.execute('''
            SELECT item_name, revenue, orders
            FROM gmv_data
            ORDER BY revenue DESC
            LIMIT 10
        ''')
        
        rows = cursor.fetchall()
        conn.close()
        
        data = []
        for row in rows:
            name = row['item_name'] or 'N/A'
            if len(name) > 30:
                name = name[:30] + '...'
            data.append({
                'name': name,
                'revenue': row['revenue'] or 0,
                'orders': row['orders'] or 0
            })
        
        return jsonify({'success': True, 'data': data})
    
    @app.route('/api/item-analytics/<item_id>')
    def api_item_analytics(item_id):
        conn = get_db()
        cursor = conn.cursor()
        
        cursor.execute('''
            SELECT item_name, revenue, clicks, file_name, session_name
            FROM raw_session_data
            WHERE item_id = ?
        ''', (item_id,))
        
        rows = cursor.fetchall()
        conn.close()
        
        if not rows:
            return jsonify({'success': False, 'error': 'Kh√¥ng t√¨m th·∫•y Item ID'})
        
        sessions = []
        total_revenue = 0
        total_clicks = 0
        item_name = ''
        
        for row in rows:
            if not item_name and row['item_name']:
                item_name = row['item_name']
            
            sessions.append({
                'session': row['session_name'] or 'N/A',
                'file': row['file_name'] or 'N/A',
                'revenue': row['revenue'] or 0,
                'clicks': row['clicks'] or 0
            })
            total_revenue += row['revenue'] or 0
            total_clicks += row['clicks'] or 0
        
        # Fallback to gmv_data
        if not item_name:
            conn2 = get_db()
            cursor2 = conn2.cursor()
            cursor2.execute('SELECT item_name FROM gmv_data WHERE item_id = ?', (item_id,))
            gmv_row = cursor2.fetchone()
            conn2.close()
            if gmv_row and gmv_row['item_name']:
                item_name = gmv_row['item_name']
        
        return jsonify({
            'success': True,
            'item_id': item_id,
            'item_name': item_name,
            'total_revenue': total_revenue,
            'total_clicks': total_clicks,
            'session_count': len(sessions),
            'sessions': sessions
        })
    
    # Return app for gunicorn
    return app

def run_web_mode():
    """Ch·∫°y Flask app v·ªõi development server"""
    app = create_app()
    port = int(os.environ.get('PORT', 5000))
    debug = os.environ.get('FLASK_DEBUG', 'true').lower() == 'true'
    app.run(host='0.0.0.0', port=port, debug=debug)


# ============== Module Level App (for gunicorn) ==============
# Gunicorn import module v√† t√¨m 'app' - ch·ªâ t·∫°o khi ƒë∆∞·ª£c import, kh√¥ng ph·∫£i khi ch·∫°y tr·ª±c ti·∫øp
if __name__ != '__main__':
    app = create_app()

# ============== Main Entry Point ==============

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='GMV App - Scraper + Web Dashboard')
    parser.add_argument('--mode', '-m', choices=['gui', 'web'], default='gui',
                        help='Mode: gui (Scraper GUI) ho·∫∑c web (Flask Server)')
    parser.add_argument('--web', '-w', action='store_true',
                        help='Shortcut cho --mode web')
    parser.add_argument('--gui', '-g', action='store_true',
                        help='Shortcut cho --mode gui')
    
    args = parser.parse_args()
    
    # X√°c ƒë·ªãnh mode
    if args.web:
        mode = 'web'
    elif args.gui:
        mode = 'gui'
    else:
        mode = args.mode
    
    print(f"üöÄ GMV App - Starting in {mode.upper()} mode...")
    
    if mode == 'web':
        run_web_mode()
    else:
        run_gui_mode()
